{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/safwanny1/apples-and-oranges/blob/main/applesandoranges.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cb21FJuVDb10",
        "outputId": "11f57f7f-24ac-47d9-90c8-7de6f64544f0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Attempting to download the latest dataset: balraj98/apple2orange-dataset\n",
            "Downloading from https://www.kaggle.com/api/v1/datasets/download/balraj98/apple2orange-dataset?dataset_version_number=1...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 74.8M/74.8M [00:00<00:00, 175MB/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting files...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset successfully downloaded to: /root/.cache/kagglehub/datasets/balraj98/apple2orange-dataset/versions/1\n"
          ]
        }
      ],
      "source": [
        "# Core Libraries\n",
        "import shutil\n",
        "import os\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# TensorFlow and Keras\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.preprocessing import image\n",
        "\n",
        "# KaggleHub for dataset download\n",
        "import kagglehub\n",
        "\n",
        "# Define datasert details\n",
        "kaggle_dataset_id = \"balraj98/apple2orange-dataset\"\n",
        "local_dataset_name = \"./AppleOrange_dataset/\" # Keeps consistent with block 2\n",
        "\n",
        "try:\n",
        "  # Download latest version\n",
        "  print(f\"Attempting to download the latest dataset: {kaggle_dataset_id}\")\n",
        "  path = kagglehub.dataset_download(kaggle_dataset_id)\n",
        "  print(f\"Dataset successfully downloaded to: {path}\")\n",
        "except Exception as e:\n",
        "  print(f\"Error downloading dataset: {e}\")\n",
        "  # Handles the error as appropriate, e.g., exit the script\n",
        "  exit()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WXZDwWL-GmIF",
        "outputId": "0fec4b91-fe8a-4af9-dc68-3060d15b6c2c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Organizing dataset from /root/.cache/kagglehub/datasets/balraj98/apple2orange-dataset/versions/1 to ././AppleOrange_dataset//\n",
            "Copying directory /root/.cache/kagglehub/datasets/balraj98/apple2orange-dataset/versions/1/testB to ././AppleOrange_dataset//testing_set/testB\n",
            "Copying directory /root/.cache/kagglehub/datasets/balraj98/apple2orange-dataset/versions/1/trainA to ././AppleOrange_dataset//training_set/trainA\n",
            "Copying directory /root/.cache/kagglehub/datasets/balraj98/apple2orange-dataset/versions/1/trainB to ././AppleOrange_dataset//training_set/trainB\n",
            "Copying directory /root/.cache/kagglehub/datasets/balraj98/apple2orange-dataset/versions/1/testA to ././AppleOrange_dataset//testing_set/testA\n",
            "Dataset organization complete. Saved at: ././AppleOrange_dataset//\n"
          ]
        }
      ],
      "source": [
        "# Use the local_dataset_name variable from block 1\n",
        "local_path = f\"./{local_dataset_name}/\"\n",
        "\n",
        "# Ensure the local directory exists\n",
        "os.makedirs(local_path, exist_ok = True)\n",
        "\n",
        "# Define the path for the new training_set and testing_set directories\n",
        "training_set_dir_name = \"training_set\"\n",
        "testing_set_dir_name = \"testing_set\"\n",
        "\n",
        "training_set_path = os.path.join(local_path, training_set_dir_name)\n",
        "testing_set_path = os.path.join(local_path, testing_set_dir_name)\n",
        "\n",
        "# Ensure the training_set and testing_set directories exist\n",
        "os.makedirs(training_set_path, exist_ok = True)\n",
        "os.makedirs(testing_set_path, exist_ok = True)\n",
        "\n",
        "# Mapping the source directory names to destination directory names\n",
        "dataset_structure_map = {\n",
        "    \"trainA\" : training_set_path,\n",
        "    \"trainB\" : training_set_path,\n",
        "    \"testA\" : testing_set_path,\n",
        "    \"testB\" : testing_set_path,\n",
        "  }\n",
        "\n",
        "# Copy the dataset to the local directory\n",
        "print(f\"Organizing dataset from {path} to {local_path}\")\n",
        "for item in os.listdir(path):\n",
        "  s = os.path.join(path, item)\n",
        "  d = dataset_structure_map.get(item) # Use .get() for safe access\n",
        "\n",
        "  # Only attempt to copy if the item is in our mapping\n",
        "  if d is not None:\n",
        "    try:\n",
        "      if os.path.isdir(s):\n",
        "        # Create the subdirectory within the destination path\n",
        "        dest_subdir = os.path.join(d, item)\n",
        "        print(f\"Copying directory {s} to {dest_subdir}\")\n",
        "        shutil.copytree(s, dest_subdir, dirs_exist_ok = True)\n",
        "      else:\n",
        "        # For files directly in the downloaded root we want to copy\n",
        "        print(f\"Copying file {s} to {d}\")\n",
        "        shutil.copy2(s, d)\n",
        "    except Exception as e:\n",
        "      print(f\"Error copying {item}: {e}\")\n",
        "\n",
        "print(f\"Dataset organization complete. Saved at: {local_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DojwV-ufHFnQ",
        "outputId": "83fe5197-7b49-464d-d82e-3f7545132af4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 2014 images belonging to 2 classes.\n",
            "Found 514 images belonging to 2 classes.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')\n",
        "\n",
        "# Define the base path using the variable from Block 1\n",
        "base_path = f\"/content/{local_dataset_name}/\"\n",
        "\n",
        "# Correcting paths using os.path.join\n",
        "train_dir = os.path.join(base_path, training_set_dir_name)\n",
        "test_dir = os.path.join(base_path, testing_set_dir_name)\n",
        "\n",
        "# Define ImageDataGenerator for augmentation and rescaling\n",
        "# Rescale the pixel values to be between 0 and 1\n",
        "# Apply data augemtation to the training set to improve robustness\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    rotation_range=20, # Rotate images by up to 20 degrees\n",
        "    width_shift_range=0.2, # Shift image horizontally by up to 20% of the width\n",
        "    height_shift_range=0.2, # Shift image vertically by up to 20% of the height\n",
        "    shear_range=0.2, # Apply shearing transformation\n",
        "    zoom_range=0.2, # Apply zoom transformation\n",
        "    horizontal_flip=True, # Flip images horizontally\n",
        "    fill_mode='nearest' # Fill newly created pixels after transformations\n",
        "    # You can experiment with adding other augmentation within this section\n",
        "    # e.g brigntess_range, channel_shift_range\n",
        ")\n",
        "\n",
        "# Only rescale the test set\n",
        "test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "# Load training dataset\n",
        "# Target size of 150x150 pixels\n",
        "# Batch size of 32 used for training\n",
        "# class_mode = 'binary' is use for binary classification\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    train_dir,\n",
        "    target_size=(150, 150),\n",
        "    batch_size=32,\n",
        "    class_mode = 'binary'\n",
        "    # You may adjust batch_size depending on memory\n",
        ")\n",
        "\n",
        "# Load testing dataset\n",
        "test_generator = test_datagen.flow_from_directory(\n",
        "    test_dir,\n",
        "    target_size=(150, 150),\n",
        "    batch_size=32,\n",
        "    class_mode='binary'\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8JeP7U5XH9qT",
        "outputId": "7c9bd07b-a266-498d-ae4e-b161d840f9e9"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        }
      ],
      "source": [
        "# Define the Sequential model\n",
        "model = tf.keras.models.Sequential([\n",
        "    # First convolutional layer with 32 filters, 3x3 kernel, ReLU activation, and input shape\n",
        "    tf.keras.layers.Conv2D(32, (3,3), activation='relu', input_shape=(150, 150, 3)),\n",
        "    # First max pooling layer\n",
        "    tf.keras.layers.MaxPooling2D(2,2),\n",
        "\n",
        "    # Second convolutional layer with 64 filters, 3x3 kernel, ReLU activation\n",
        "    tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n",
        "    # Second max pooling layer\n",
        "    tf.keras.layers.MaxPooling2D(2,2),\n",
        "\n",
        "    # Third convolutional layer with 128 filters, 3x3 kernel, ReLU activation\n",
        "    tf.keras.layers.Conv2D(128, (3,3), activation='relu'),\n",
        "    # Third max pooling layer\n",
        "    tf.keras.layers.MaxPooling2D(2,2),\n",
        "\n",
        "    # Flatten the output from the convolutional layers\n",
        "    tf.keras.layers.Flatten(),\n",
        "    # First dense layer with 512 units and ReLU activation\n",
        "    tf.keras.layers.Dense(512, activation='relu'),\n",
        "    # Output dense layer with 1 unit and sigmoid activation for binary classification\n",
        "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "# Use binary crossentropy as the loss function for binary classification\n",
        "# Use the Adam optimizer (consider experimenting with other optimizers and learning rates)\n",
        "# Evaluate the model based on accuracy\n",
        "model.compile(loss='binary_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "cBj3nOSoIAdT",
        "outputId": "208eebaf-f301-4023-eaf7-9642f2fd4c33"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting model training...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
            "  self._warn_if_super_not_called()\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m130s\u001b[0m 2s/step - accuracy: 0.7089 - loss: 0.7541 - val_accuracy: 0.8969 - val_loss: 0.2671\n",
            "Epoch 2/5\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m123s\u001b[0m 2s/step - accuracy: 0.9096 - loss: 0.1979 - val_accuracy: 0.9280 - val_loss: 0.2100\n",
            "Epoch 3/5\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m125s\u001b[0m 2s/step - accuracy: 0.9370 - loss: 0.1714 - val_accuracy: 0.9222 - val_loss: 0.2365\n",
            "Epoch 4/5\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m122s\u001b[0m 2s/step - accuracy: 0.9451 - loss: 0.1607 - val_accuracy: 0.9241 - val_loss: 0.2216\n",
            "Epoch 5/5\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m128s\u001b[0m 2s/step - accuracy: 0.9305 - loss: 0.1789 - val_accuracy: 0.9300 - val_loss: 0.2006\n",
            "Model training has finished\n"
          ]
        },
        {
          "ename": "NameError",
          "evalue": "name 'model_path' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-22ecfe200aa2>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m# Saves model to h5 file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'model_path' is not defined"
          ]
        }
      ],
      "source": [
        "# Train the model using the data generators\n",
        "# Train for a specified number of epochs (consider increasing this for better results)\n",
        "# Validate the model during training using the testing data\n",
        "print (\"Starting model training...\")\n",
        "history = model.fit(\n",
        "    train_generator,\n",
        "    epochs=5,\n",
        "    validation_data=test_generator\n",
        ")\n",
        "print (\"Model training has finished\")\n",
        "\n",
        "# Saves model to h5 file\n",
        "model.save(model_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Xz96QgvCsw-E"
      },
      "outputs": [],
      "source": [
        "# === Visualization of training results ===\n",
        "print(\"Visualizing training results...\")\n",
        "\n",
        "# Create a figure with two subplots for accuracy and loss\n",
        "plt.figure(figsize=(12,5))\n",
        "\n",
        "# Plot training and validation accuracy\n",
        "plt.subplot(1,2,1)\n",
        "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
        "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
        "plt.title('Model Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "\n",
        "# Plot training and validation loss\n",
        "plt.subplot(1,2,2)\n",
        "plt.plot(history.history['loss'], label='Training Loss')\n",
        "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "plt.title('Model Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "\n",
        "# Adjust layout to prevent overlap and display the plot\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "ixYomdmLXusP"
      },
      "outputs": [],
      "source": [
        "# Model evaulation\n",
        "print (\"Evaluating model on test set...\")\n",
        "test_loss, test_acc = model.evaluate(test_generator)\n",
        "print(f\"Test Loss: {test_loss:.4f}\")\n",
        "print(f\"Test Accuracy: {test_acc:.2f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "p1dp0PU5IhBw"
      },
      "outputs": [],
      "source": [
        "# Load the pre-trained Keras model\n",
        "model = load_model(model_path)\n",
        "\n",
        "\n",
        "def predict_image(img_path):\n",
        "    \"\"\"\n",
        "    Predicts the class of a single image (Apple or Orange) and displays the image\n",
        "    with the prediction.\n",
        "\n",
        "    Args:\n",
        "        img_path (str): The path to the image file.\n",
        "    \"\"\"\n",
        "    if not os.path.exists(img_path):\n",
        "        print(f\"Error: Image not found at {img_path}\")\n",
        "        return\n",
        "\n",
        "    try:\n",
        "        # Load the image and resize it to the target size\n",
        "        img = image.load_img(img_path, target_size=(150, 150))\n",
        "        # Convert the image to a NumPy array\n",
        "        img_array = image.img_to_array(img)\n",
        "        # Expand the dimensions to create a batch of size 1 and normalize the pixel values\n",
        "        img_array = np.expand_dims(img_array, axis=0) / 255.0\n",
        "        # Make a prediction using the trained model\n",
        "        prediction = model.predict(img_array)\n",
        "        # Determine the label based on the prediction threshold (0.5 for binary classification)\n",
        "        label = \"Orange\" if prediction[0][0] > 0.5 else \"Apple\"\n",
        "\n",
        "        # Display the image with the predicted label\n",
        "        plt.imshow(img)\n",
        "        plt.axis(\"off\")\n",
        "        plt.title(f\"Prediction: {label}\", fontsize=14, fontweight='bold')\n",
        "        plt.show()\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing image {img_path}: {e}\")\n",
        "\n",
        "# Specify the path to an image for prediction\n",
        "img_path = \"/content/AppleOrange_dataset/testing_set/testA/n07740461_11260.jpg\"\n",
        "# Call the predict_image function to predict and display the result\n",
        "predict_image(img_path)\n",
        "# The function itself displays the image and prediction, so no print is needed here."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}